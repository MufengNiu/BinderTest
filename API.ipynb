{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XctX73JmCLi",
    "outputId": "1831f3f8-271d-40ea-d2f7-4af96b95fbff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import certifi\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import FixedLocator\n",
    "import json\n",
    "\n",
    "\n",
    "api_key = \"118322912aa8543272e4c9f2401f3a084cf4b1c7f0ff0b2809955ed44a96050a\"\n",
    "excludeWords = [\"[\",\"\\\\\", \"]\", \"_\", \"`\", \"!\", \"\\\"\", \"#\", \"%\", \"'\", \"(\", \")\", \"+\", \",\", \"-\", \"–\", \".\", \"/\", \":\", \";\", \"{\", \"|\", \"}\", \"=\", \"~\", \"?\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eZ-1ptCm6G5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List All Text Sets\n",
    "url = \"https://intelligentarchive.sydney.edu.au/api/v1/text-sets\"\n",
    "\n",
    "response = requests.get(url, headers={\"X-API-KEY\": api_key})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse JSON response into Python object\n",
    "    text_sets = response.json()\n",
    "    \n",
    "    # Loop through each text set to print its ID and name\n",
    "    print(\"List of Text Sets:\")\n",
    "    for text_set in text_sets:\n",
    "        set_id = text_set.get('id', 'N/A')  # 'N/A' will be used if 'id' is not available\n",
    "        set_name = text_set.get('name', 'N/A')  # 'N/A' will be used if 'name' is not available\n",
    "        print(f\"  - Name: {set_name}   ID: {set_id}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to get data: {response.status_code} {response.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 579
    },
    "id": "5AhZk0xJrW_y",
    "outputId": "8d9753e5-f187-4eac-f7e2-ec1d670b0240",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Top 20 most frequent words\n",
    "\n",
    "textset_id = 86  # Replace with your actual text set ID\n",
    "\n",
    "# URL and Data for Word Frequencies\n",
    "url = \"https://intelligentarchive.sydney.edu.au/api/v1/word-frequencies\"\n",
    "word_freq_request = {\n",
    "    'textSet': textset_id,\n",
    "    'option' :{\n",
    "        'outputSize' : 30\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the API Request\n",
    "response = requests.post(url, json=word_freq_request, headers={\"X-API-KEY\": api_key}, timeout=1200)\n",
    "\n",
    "# Create a dictionary to hold word frequencies\n",
    "word_frequency_map = {}\n",
    "\n",
    "# Handle the Response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    blocks = response_data.get(\"blocks\", [])\n",
    "\n",
    "    for block in blocks:\n",
    "        freqs = block.get('frequencies', [])\n",
    "\n",
    "        for freq in freqs:\n",
    "            word = freq.get('word')\n",
    "            value = freq.get('value')\n",
    "\n",
    "            if word not in excludeWords:\n",
    "             word_frequency_map[word] = word_frequency_map.get(word, 0) + value\n",
    "\n",
    "    # Sort by frequency and take the top 20 words\n",
    "    sorted_items = sorted(word_frequency_map.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    sorted_words = [item[0] for item in sorted_items]\n",
    "    sorted_frequencies = [item[1] for item in sorted_items]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sorted_words, sorted_frequencies, color='blue')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('20 Most Frequent Words')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"Failed: {response.status_code} {response.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX1: The top 10 6-grams in the 20 plays by frequency\n",
    "\n",
    "# Text set = Shakespeare 20 for demos\n",
    "# Segment by text\n",
    "# Ngrams = 6\n",
    "# Output Size = 10\n",
    "\n",
    "textset_id = 86  # Replace with your actual text set ID\n",
    "\n",
    "# URL and Data for Word Frequencies\n",
    "url = \"https://intelligentarchive.sydney.edu.au/api/v1/word-frequencies\"\n",
    "word_freq_request = {\n",
    "    'textSet': textset_id,\n",
    "    'option':{\n",
    "        'numberOfNGrams' : 6,\n",
    "        'outputSize': 10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the API Request\n",
    "response = requests.post(url, json=word_freq_request, headers={\"X-API-KEY\": api_key}, timeout=1200)\n",
    "\n",
    "# Create a dictionary to hold word frequencies\n",
    "word_frequency_map = {}\n",
    "\n",
    "# Handle the Response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    blocks = response_data.get(\"blocks\", [])\n",
    "\n",
    "    for block in blocks:\n",
    "        freqs = block.get('frequencies', [])\n",
    "\n",
    "        for freq in freqs:\n",
    "            word = freq.get('word').replace('.' , ' ')\n",
    "            value = freq.get('value')\n",
    "\n",
    "            word_frequency_map[word] = word_frequency_map.get(word, 0) + value\n",
    "\n",
    "    # Sort by frequency and take the top 20 words\n",
    "    sorted_items = sorted(word_frequency_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_words = [item[0] for item in sorted_items]\n",
    "    sorted_frequencies = [item[1] for item in sorted_items]\n",
    "\n",
    "    # # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sorted_words, sorted_frequencies, color='blue')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 10 6-grams in the 20 plays by frequency ')\n",
    "    plt.xticks(rotation=70)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # # Create a table\n",
    "    # fig, ax = plt.subplots(figsize=(12, 3))\n",
    "    # ax.axis('tight')\n",
    "    # ax.axis('off')\n",
    "    # table_data = [[\"Word\"] + sorted_words,\n",
    "    #               [\"Frequency\"] + sorted_frequencies]\n",
    "    # the_table = ax.table(cellText=table_data, colLabels=None, cellLoc = 'center', loc='center')\n",
    "\n",
    "    # # Make cells larger to fit long words\n",
    "    # the_table.auto_set_font_size(False)\n",
    "    # the_table.set_fontsize(10)\n",
    "    # the_table.scale(2.0, 2.0)\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"Failed: {response.status_code} {response.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EX2: The size of character parts from largest to smallest in these 20 plays \n",
    "\n",
    "# Text set = Shakespeare 20 for demos \n",
    "# Segment by text and by character \n",
    "# Top 10 words \n",
    "# Output – n=640 \n",
    "# Transform and chart – sort by Size, largest to smallest. Chart this row as a scatterplot. \n",
    "\n",
    "# Note From Hugh  \n",
    "\n",
    "textset_id = 86  # Replace with your actual text set ID \n",
    "\n",
    "\n",
    "character_parts_request = {\n",
    "    'textSet': textset_id,\n",
    "    'option': {\n",
    "        # 'segmentByCharacter': True,\n",
    "        'outputSize': 660  \n",
    "    }\n",
    "}\n",
    "\n",
    "# URL\n",
    "url = \"https://intelligentarchive.sydney.edu.au/api/v1/word-frequencies\"\n",
    "\n",
    "# Make API request\n",
    "response = requests.post(url, json=character_parts_request, headers={\"X-API-KEY\": api_key}, timeout=1200)\n",
    "\n",
    "# Create a dictionary to hold character frequencies\n",
    "character_frequency_map = {}\n",
    "\n",
    "# Handle the response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    blocks = response_data.get(\"blocks\", [])\n",
    "\n",
    "    for block in blocks:\n",
    "        freqs = block.get('frequencies', [])\n",
    "\n",
    "        for freq in freqs:\n",
    "            word = freq.get('word' , \"Unknown\")\n",
    "            word_count = freq.get('value', 0)\n",
    "            \n",
    "            if word not in excludeWords:\n",
    "                character_frequency_map[word] = character_frequency_map.get(word, 0) + word_count\n",
    "\n",
    "    # Sort by size and take the top 640 characters\n",
    "    sorted_characters = sorted(character_frequency_map.items(), key=lambda x: x[1], reverse=True)[:640]\n",
    "    names = [item[0] for item in sorted_characters]\n",
    "    sizes = [item[1] for item in sorted_characters]\n",
    "\n",
    "    # Scatterplot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(names, sizes, c='blue')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('Character Names')\n",
    "    plt.ylabel('Size of Spoken Part')\n",
    "    plt.title('Size of Character Parts in 20 Shakespeare Plays')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"Failed: {response.status_code} {response.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Frequencies of HAS and HATH in the plays \n",
    "\n",
    "# Text set = Shakespeare 20 for demos \n",
    "# Segment by text \n",
    "# Include metadata in output \n",
    "# Words unsorted – HAS HATH \n",
    "# Output – choose proportions \n",
    "# Transform and chart – sort by DATE, plot HAS and HATH as lines \n",
    "\n",
    "textset_id = 86  # Replace with your actual text set ID \n",
    "\n",
    "\n",
    "character_parts_request = {\n",
    "    'textSet': textset_id,\n",
    "    'option': {\n",
    "        'segmentByCharacter': False,\n",
    "        'outputSize': 1000 , \n",
    "         #'outputSpecialWords' : [\"has\",\"hath\"],\n",
    "        'outputSpecialWordsOption' : 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# URL\n",
    "url = \"https://intelligentarchive.sydney.edu.au/api/v1/word-frequencies\"\n",
    "\n",
    "# Make API request\n",
    "response = requests.post(url, json=character_parts_request, headers={\"X-API-KEY\": api_key}, timeout=1200)\n",
    "\n",
    "special_word_count_by_year = {\"has\": {}, \"hath\": {}}\n",
    "total_word_count_by_year = {}\n",
    "\n",
    "# Handle the response\n",
    "if response.status_code == 200:\n",
    "    response_data = response.json()\n",
    "    blocks = response_data.get(\"blocks\", [])\n",
    "    \n",
    "    for block in blocks:\n",
    "        name = block.get(\"name\", \"\")\n",
    "        freqs = block.get(\"frequencies\", [])\n",
    "        \n",
    "        # Extract year from name\n",
    "        year = name.split(\"_\")[-1].split(\" \")[0]\n",
    "        \n",
    "        for freq in freqs:\n",
    "            word = freq.get(\"word\", \"\")\n",
    "            value = freq.get(\"value\", 0)\n",
    "            \n",
    "            if word.lower() == \"has\":\n",
    "                special_word_count_by_year[\"has\"][year] = special_word_count_by_year[\"has\"].get(year, 0) + value\n",
    "            elif word.lower() == \"hath\":\n",
    "                special_word_count_by_year[\"hath\"][year] = special_word_count_by_year[\"hath\"].get(year, 0) + value\n",
    "                \n",
    "            total_word_count_by_year[year] = total_word_count_by_year.get(year, 0) + value\n",
    "\n",
    "    # Sort by Year\n",
    "    sorted_years = sorted(set(special_word_count_by_year[\"has\"].keys()) | set(special_word_count_by_year[\"hath\"].keys()))\n",
    "    has_counts = [(special_word_count_by_year[\"has\"].get(year, 0) / total_word_count_by_year.get(year, 1)) * 100 for year in sorted_years]\n",
    "    hath_counts = [(special_word_count_by_year[\"hath\"].get(year, 0) / total_word_count_by_year.get(year, 1)) * 100 for year in sorted_years]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(sorted_years, has_counts, label='HAS', marker='o')\n",
    "    plt.plot(sorted_years, hath_counts, label='HATH', marker='o')\n",
    "    plt.scatter(sorted_years, has_counts)\n",
    "    plt.scatter(sorted_years, hath_counts)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Percentage of \"HAS\" and \"HATH\" over years')\n",
    "\n",
    "    y_ticks = plt.gca().get_yticks()\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(y_ticks))\n",
    "    plt.gca().set_yticklabels(['{:.2f}%'.format(y) for y in y_ticks])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Failed: {response.status_code} {response.reason}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
